{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96916e77",
   "metadata": {},
   "source": [
    "# Model 2: U-net\n",
    "\n",
    "In this notebook you will find the second (the main) model (after some development iterations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a41fd53",
   "metadata": {},
   "source": [
    "## Project setup\n",
    "\n",
    "### Library setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085958c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, Model # type: ignore\n",
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError # type: ignore\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from tensorflow.keras.models import load_model  # type: ignore\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img # type: ignore\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea0bef",
   "metadata": {},
   "source": [
    "### Weights and Biases integration\n",
    "\n",
    "In this step, the program will ask for an API key which is unique to each program or user.\n",
    "To integrate this platform to this run, please configure it with your `entity`, `project` and `name` and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "wandb.init(\n",
    "    entity=\"mehher_ghevandiani-american-university-of-armenia\",\n",
    "    project=\"Capstone\",\n",
    "    name=\"U-net model iteration 5\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"U-Net\",\n",
    "        \"dataset\": \"combined dataset\",\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"WandB initiated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c57ee0b",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, img_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Reads an image, converts it to LAB color space, resizes it, and normalizes the channels.\n",
    "\n",
    "    Parameters:\n",
    "        img_path (str): Path to the image file.\n",
    "        img_size (tuple): Desired image size (width, height). Default is (256, 256).\n",
    "\n",
    "    Returns:\n",
    "        l_channel (np.ndarray): Normalized L channel in range [0, 1].\n",
    "        ab_channels (np.ndarray): Normalized AB channels in range [-1, 1].\n",
    "\n",
    "    Note:\n",
    "        If the image cannot be read, returns None and prints an error message.\n",
    "    \"\"\"\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        l_channel = img[:, :, 0:1] / 255.0  # Normalize L channel\n",
    "        ab_channels = (img[:, :, 1:] - 128) / 128.0  # Normalize AB channels\n",
    "        return l_channel, ab_channels\n",
    "    else:\n",
    "        print(f\"Image at {img_path} could not be read.\")\n",
    "\n",
    "def preprocess_images(data_dir, img_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Traverses a directory to preprocess all images using LAB color space conversion.\n",
    "\n",
    "    Parameters:\n",
    "        data_dir (str): Directory path containing image files (recursively searched).\n",
    "        img_size (tuple): Target size for resizing each image. Default is (256, 256).\n",
    "\n",
    "    Returns:\n",
    "        l_channels (np.ndarray): Stack of normalized L channels for all images.\n",
    "        ab_channels (np.ndarray): Stack of normalized AB channels for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    l_channels = []\n",
    "    ab_channels = []\n",
    "\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        print(\"Processing directory:\", root)\n",
    "        for file in files:\n",
    "            print(\"Processing file:\", file)\n",
    "            img_path = os.path.join(root, file)\n",
    "            l_channel, ab_channel = preprocess_image(img_path, img_size)\n",
    "            l_channels.append(l_channel)\n",
    "            ab_channels.append(ab_channel)\n",
    "\n",
    "    l_channels = np.array(l_channels, dtype=np.float32)\n",
    "    ab_channels = np.array(ab_channels, dtype=np.float32)\n",
    "\n",
    "    return l_channels, ab_channels\n",
    "\n",
    "\n",
    "def lab_to_rgb(l_channel, ab_channels):\n",
    "    \"\"\"\n",
    "    Converts normalized LAB image data back into RGB format.\n",
    "\n",
    "    Parameters:\n",
    "        l_channel (np.ndarray): L channel with values in [0, 1].\n",
    "        ab_channels (np.ndarray): AB channels with values in [-1, 1].\n",
    "\n",
    "    Returns:\n",
    "        rgb_image (np.ndarray): Image converted to RGB format.\n",
    "    \"\"\"\n",
    "\n",
    "    l_channel = (l_channel * 255).astype(np.uint8)\n",
    "    ab_channels = (ab_channels * 128 + 128).astype(np.uint8)\n",
    "    lab_image = np.concatenate((l_channel, ab_channels), axis=-1)\n",
    "    rgb_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2RGB)\n",
    "    return rgb_image\n",
    "\n",
    "def prepare_data(data_dir):\n",
    "    \"\"\"\n",
    "    Preprocesses all images in a given directory and separates them into input (L) and label (AB) data.\n",
    "\n",
    "    Parameters:\n",
    "        data_dir (str): Path to the directory containing image data.\n",
    "\n",
    "    Returns:\n",
    "        images (np.ndarray): Array of normalized L channel data.\n",
    "        labels (np.ndarray): Array of normalized AB channel data.\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing images\")\n",
    "    images, labels = preprocess_images(data_dir)\n",
    "    print(\"Data Preprocessed\")\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deded709",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Below you will also find the function responcible for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_shape=(256, 256, 1)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Rescaling(1./255)(inputs)  # Normalize grayscale input\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    c1 = layers.BatchNormalization()(c1)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c1)\n",
    "    c1 = layers.BatchNormalization()(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.BatchNormalization()(c2)\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c2)\n",
    "    c2 = layers.BatchNormalization()(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.BatchNormalization()(c3)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c3)\n",
    "    c3 = layers.BatchNormalization()(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = layers.BatchNormalization()(c4)\n",
    "    c4 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c4)\n",
    "    c4 = layers.BatchNormalization()(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)\n",
    "    c5 = layers.BatchNormalization()(c5)\n",
    "    c5 = layers.Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)\n",
    "    c5 = layers.BatchNormalization()(c5)\n",
    "\n",
    "    # Decoder\n",
    "    u6 = layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(u6)\n",
    "    c6 = layers.BatchNormalization()(c6)\n",
    "    c6 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c6)\n",
    "    c6 = layers.BatchNormalization()(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u7)\n",
    "    c7 = layers.BatchNormalization()(c7)\n",
    "    c7 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c7)\n",
    "    c7 = layers.BatchNormalization()(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u8)\n",
    "    c8 = layers.BatchNormalization()(c8)\n",
    "    c8 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c8)\n",
    "    c8 = layers.BatchNormalization()(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u9)\n",
    "    c9 = layers.BatchNormalization()(c9)\n",
    "    c9 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c9)\n",
    "    c9 = layers.BatchNormalization()(c9)\n",
    "\n",
    "    outputs = layers.Conv2D(2, (1, 1), activation='tanh')(c9)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def train_model(images, labels, callbacks=None):\n",
    "    \"\"\"\n",
    "    Trains a U-Net model on the given dataset using LAB color space inputs and saves the trained model.\n",
    "\n",
    "    Parameters:\n",
    "        images (np.ndarray): Input data, typically L channel images with shape (num_samples, height, width, 1).\n",
    "        labels (np.ndarray): Target data, typically AB channels with shape (num_samples, height, width, 2).\n",
    "        callbacks (list, optional): List of Keras callbacks to apply during training. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        model (tf.keras.Model): The trained U-Net model.\n",
    "\n",
    "    Note:\n",
    "        - Uses global variables: `learning_rate`, `batch_size`, and `epochs`.\n",
    "        - The trained model is saved to a file named using the number of epochs (e.g., 'u_net_50_epoch_iter_5.h5').\n",
    "    \"\"\"\n",
    "    model = unet_model()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='mae',\n",
    "        metrics=[MeanSquaredError(), MeanAbsoluteError()]\n",
    "    )\n",
    "    print(\"Model Compiled\")\n",
    "\n",
    "    print(\"Fitting The Model\")\n",
    "\n",
    "    model.fit(\n",
    "        images, labels,\n",
    "        validation_split=0.2,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    model.save(f'u_net_{epochs}_epoch_iter_5.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7080f7",
   "metadata": {},
   "source": [
    "## The training process\n",
    "\n",
    "### Images and Labels array setup\n",
    "\n",
    "This is the part where we load or get the images and labels arrays. If the files images.npy and labels.npy already exist, they will get loaded, if not, they will get created by the \n",
    "`prepare_data()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../../train_data\" # Refer to the readme if you haven't downloaded the dataset yet\n",
    "\n",
    "try:\n",
    "    if os.path.exists(\"./images.npy\") and os.path.exists(\"./labels.npy\"):\n",
    "        print(\"Loading images and labels from .npy files\")\n",
    "        images = np.load(\"./images.npy\")\n",
    "        labels = np.load(\"./labels.npy\")\n",
    "except FileNotFoundError:   \n",
    "    print(\"Couldn't find .npy files, preprocessing images from dataset directory\")\n",
    "    images, labels = prepare_data(dataset_dir)\n",
    "    np.save(\"./images.npy\", images)\n",
    "    np.save(\"./labels.npy\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='u_net_colorization_checkpoint.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint_callback,\n",
    "              WandbMetricsLogger()\n",
    "              ]\n",
    "\n",
    "model = train_model(images, labels, callbacks=callbacks) # be cautios to run, it will take a while\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2796e2c",
   "metadata": {},
   "source": [
    "## Testing the results\n",
    "\n",
    "This part is responsible for plotting the visualisation results of the model.\n",
    "For research purposes, all of the images in the dataset will be colorized, feel free to interrupt the process with `cntrl+c`. In this case you will see a KeyboardInterrupt error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063bdf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(f'u_net_{epochs}_epoch_iter_5.h5', compile=False) # Uncomment this line to load a pre-trained model\n",
    "\n",
    "nighttime_dir = \"../../nighttime_footage\" # a sample dataset to test the model\n",
    "\n",
    "for filename in os.listdir(dataset_dir): # change to nighttime_dir to test on the nighttime dataset\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(dataset_dir, filename)\n",
    "\n",
    "        original_image = load_img(image_path)\n",
    "        original_image = img_to_array(original_image) / 255.0\n",
    "        l_channel, ab_channels = preprocess_image(image_path)\n",
    "\n",
    "        l_channel_input = np.expand_dims(l_channel, axis=0)\n",
    "        predicted_ab_channels = model.predict(l_channel_input)[0]\n",
    "\n",
    "        colorized_image = lab_to_rgb(l_channel, predicted_ab_channels)\n",
    "        grayscale_image = l_channel[:, :, 0]\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(f\"Original Image: {filename}\")\n",
    "        plt.imshow(original_image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Grayscale Image\")\n",
    "        plt.imshow(grayscale_image, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Colorized Image\")\n",
    "        plt.imshow(colorized_image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
