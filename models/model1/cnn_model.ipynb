{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8955615",
   "metadata": {},
   "source": [
    "# Model 1: Convolutional Neural Networks\n",
    "\n",
    "In this notebook you will find the first model (after some development iterations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab2b3d",
   "metadata": {},
   "source": [
    "## Project setup\n",
    "\n",
    "### Library setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras import Model # type: ignore\n",
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError # type: ignore\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, Input, BatchNormalization # type: ignore\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "from tensorflow.keras.models import load_model  # type: ignore\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img # type: ignore\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Libraries loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c188a",
   "metadata": {},
   "source": [
    "### Weights and Biases integration\n",
    "\n",
    "In this step, the program will ask for an API key which is unique to each program or user.\n",
    "To integrate this platform to this run, please configure it with your `entity`, `project` and `name` and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2aa4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "wandb.init(\n",
    "    entity=\"mehher_ghevandiani-american-university-of-armenia\",\n",
    "    project=\"Capstone\",\n",
    "    name=\"U-net model iteration 5\",\n",
    "    config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"architecture\": \"U-Net\",\n",
    "        \"dataset\": \"human_detection_dataset\",\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"WandB initiated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a644c3f2",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6bdfc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, img_size=(256, 256)):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2LAB).astype(np.float32)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        l_channel = img[:, :, 0:1] / 255.0  # Normalize L channel\n",
    "        ab_channels = (img[:, :, 1:] - 128) / 128.0  # Normalize AB channels\n",
    "        return l_channel, ab_channels\n",
    "    else:\n",
    "        print(f\"Image at {img_path} could not be read.\")\n",
    "\n",
    "def preprocess_images(data_dir, img_size=(256, 256)):\n",
    "    l_channels = []\n",
    "    ab_channels = []\n",
    "\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        print(\"Processing directory:\", root)\n",
    "        for file in files:\n",
    "            print(\"Processing file:\", file)\n",
    "            img_path = os.path.join(root, file)\n",
    "            l_channel, ab_channel = preprocess_image(img_path, img_size)\n",
    "            l_channels.append(l_channel)\n",
    "            ab_channels.append(ab_channel)\n",
    "\n",
    "    l_channels = np.array(l_channels, dtype=np.float32)\n",
    "    ab_channels = np.array(ab_channels, dtype=np.float32)\n",
    "\n",
    "    return l_channels, ab_channels\n",
    "\n",
    "def lab_to_rgb(l_channel, ab_channels):\n",
    "    l_channel = (l_channel * 255).astype(np.uint8)\n",
    "    ab_channels = (ab_channels * 128 + 128).astype(np.uint8)\n",
    "    lab_image = np.concatenate((l_channel, ab_channels), axis=-1)\n",
    "    rgb_image = cv2.cvtColor(lab_image, cv2.COLOR_LAB2RGB)\n",
    "    return rgb_image\n",
    "\n",
    "def prepare_data(data_dir):\n",
    "    print(\"Preprocessing images\")\n",
    "    images, labels = preprocess_images(data_dir)\n",
    "    print(\"Data Preprocessed\")\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01c2e4",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Below you will also find the function responcible for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(input_shape=(256, 256, 1)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = Conv2D(2, (3, 3), activation=\"tanh\", padding=\"same\")(x)  # 2 output channels (A, B)\n",
    "\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "def train_model(images, labels, callbacks=None):\n",
    "    model = cnn_model()\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mae', metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
    "\n",
    "    print(\"Model Compiled\")\n",
    "\n",
    "    print(\"Fitting The Model\")\n",
    "\n",
    "    model.fit(\n",
    "        images, labels,\n",
    "        validation_split=0.2,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    model.save(f'cnn_{epochs}_epochs_model.keras')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267f707b",
   "metadata": {},
   "source": [
    "## The training process\n",
    "\n",
    "### Images and Labels array setup\n",
    "\n",
    "This is the part where we load or get the images and labels arrays. If the files images.npy and labels.npy already exist, they will get loaded, if not, they will get created by the \n",
    "`prepare_data()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b789bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../../train_data\" # Refer to the readme if you haven't downloaded the dataset yet\n",
    "\n",
    "try:\n",
    "    if os.path.exists(\"./images.npy\") and os.path.exists(\"./labels.npy\"):\n",
    "        print(\"Loading images and labels from .npy files\")\n",
    "        images = np.load(\"./images.npy\")\n",
    "        labels = np.load(\"./labels.npy\")\n",
    "except FileNotFoundError:   \n",
    "    print(\"Couldn't find .npy files, preprocessing images from dataset directory\")\n",
    "    images, labels = prepare_data(dataset_dir)\n",
    "    np.save(\"./images.npy\", images)\n",
    "    np.save(\"./labels.npy\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231be672",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(images, labels) # be cautios to run, it will take a while"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d798777",
   "metadata": {},
   "source": [
    "## Testing the results\n",
    "\n",
    "This part is responsible for plotting the visualisation results of the model.\n",
    "For research purposes, all of the images in the dataset will be colorized, feel free to interrupt the process with `cntrl+c`. In this case you will see a KeyboardInterrupt error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f450ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = load_model(f'cnn_{epochs}_epochs_model.keras', compile=False) # Uncomment this line to load a pre-trained model\n",
    "\n",
    "for filename in os.listdir(dataset_dir):\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(dataset_dir, filename)\n",
    "\n",
    "        original_image = load_img(image_path)\n",
    "        original_image = img_to_array(original_image) / 255.0\n",
    "        l_channel, ab_channels = preprocess_image(image_path)\n",
    "\n",
    "        l_channel_input = np.expand_dims(l_channel, axis=0)\n",
    "        predicted_ab_channels = model.predict(l_channel_input)[0]\n",
    "\n",
    "        colorized_image = lab_to_rgb(l_channel, predicted_ab_channels)\n",
    "        grayscale_image = l_channel[:, :, 0]\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(f\"Original Image: {filename}\")\n",
    "        plt.imshow(original_image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(\"Grayscale Image\")\n",
    "        plt.imshow(grayscale_image, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(\"Colorized Image\")\n",
    "        plt.imshow(colorized_image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
